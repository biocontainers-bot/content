
{
    "credit": [
        {
            "email": "ctseng@pitt.edu",
            "name": "Tseng GC",
            "typeEntity": "Person",
            "typeRoles": [
                "Primary contact"
            ]
        }
    ],
    "documentation": [
        {
            "type": "General",
            "url": "http://tsenglab.biostat.pitt.edu/software.htm#MLbias"
        }
    ],
    "function": [
        {
            "operation": [
                {
                    "term": "Comparison",
                    "uri": "http://edamontology.org/operation_2424"
                }
            ]
        }
    ],
    "labels": {
        "language": [
            "R"
        ],
        "operatingSystem": [
            "Linux",
            "Windows",
            "Mac"
        ],
        "toolType": [
            "Command-line tool"
        ],
        "topic": [
            {
                "term": "Machine learning",
                "uri": "http://edamontology.org/topic_3474"
            }
        ]
    },
    "link": [
        {
            "type": "Software catalogue",
            "url": "http://www.mybiosoftware.com/mlbias-1-0-correct-machine-learning-bias.html"
        }
    ],
    "publication": [
        {
            "abstract": "Â© The Author 2014. Published by Oxford University Press.Motivation: Supervised machine learning is commonly applied in genomic research to construct a classifier from the training data that is generalizable to predict independent testing data. When test datasets are not available, cross-validation is commonly used to estimate the error rate. Many machine learning methods are available, and it is well known that no universally best method exists in general. It has been a common practice to apply many machine learning methods and report the method that produces the smallest cross-validation error rate. Theoretically, such a procedure produces a selection bias. Consequently, many clinical studies with moderate sample sizes (e.g. n = 30-60) risk reporting a falsely small cross-validation error rate that could not be validated later in independent cohorts.Results: In this article, we illustrated the probabilistic framework of the problem and explored the statistical and asymptotic properties. We proposed a new bias correction method based on learning curve fitting by inverse power law (IPL) and compared it with three existing methods: nested cross-validation, weighted mean correction and Tibshirani-Tibshirani procedure. All methods were compared in simulation datasets, five moderate size real datasets and two large breast cancer datasets. The result showed that IPL outperforms the other methods in bias correction with smaller variance, and it has an additional advantage to extrapolate error estimates for larger sample sizes, a practical feature to recommend whether more samples should be recruited to improve the classifier and accuracy. An R package 'MLbias' and all source files are publicly available.",
            "authors": [
                "Ding Y.",
                "Tang S.",
                "Liao S.G.",
                "Jia J.",
                "Oesterreich S.",
                "Lin Y.",
                "Tseng G.C."
            ],
            "cit_count": 9,
            "journal": "Bioinformatics",
            "pmid": "25086004",
            "title": "Bias correction for selecting the minimal-error classifier from many machine learning models",
            "year": "2014-01-01"
        }
    ],
    "summary": {
        "biotoolsCURIE": "biotools:mlbias",
        "biotoolsID": "mlbias",
        "description": "R package to correct for machine learning bias when many classifiers are compared and the best is selected.",
        "homepage": "http://tsenglab.biostat.pitt.edu/software.htm#MLbias",
        "name": "MLbias"
    }
}
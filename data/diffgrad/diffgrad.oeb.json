
{
    "credit": [
    ],
    "labels": {
        "topic": [
            {
                "term": "Machine learning",
                "uri": "http://edamontology.org/topic_3474"
            },
            {
                "term": "Imaging",
                "uri": "http://edamontology.org/topic_3382"
            },
            {
                "term": "Mathematics",
                "uri": "http://edamontology.org/topic_3315"
            }
        ]
    },
    "publication": [
        {
            "doi": "10.1109/TNNLS.2019.2955777",
            "pmid": "31880565"
        }
    ],
    "summary": {
        "biotoolsCURIE": "biotools:diffGrad",
        "biotoolsID": "diffGrad",
        "description": "An Optimization Method for Convolutional Neural Networks.\n\nAbstract Stochastic Gradient Decent (SGD) is one of the core techniques behind the success of deep neural networks",
        "homepage": "https://github.com/shivram1987/diffGrad",
        "name": "diffGrad"
    }
}